1.机器学习的内容
答案：1.机器学习概观，机器学习中超脱于具体模型和方法之上的一些共性问题，将从概率的两大派别开始。
    频率学派与贝叶斯学派对概率迥异的认知也将机器学习一分为二，发展出两套完全不同的理论体系。
     2.论频率学派发展出的机器学习理论——统计学习。统计机器学习的核心是数据，它既从数据中来，利用不同的模型去拟合数据背后的规律；
     也到数据中去，用拟合出的规律去推断和预测未知的结果。统计学习中最基础的模型是线性回归，几乎所有其他模型都是从不同角度对线性回归模型做出的扩展与修正
     3.贝叶斯学派发展出的机器学习理论——符号学习，也就是概率图模型。和基于数据的统计学习相比，基于关系的图模型更多地代表了因果推理的发展方向。
     贝叶斯主义也需要计算待学习对象的概率分布，但它利用的不是海量的具体数据，而是变量之间的相关关系、每个变量的先验分布和大量复杂的积分技巧。
     在这个模块中，将围绕概率图模型中的表示、推断、学习三大问题展开

2.机器学习的方式
答案：机器学习是一门研究通过计算的手段利用经验来改善系统自身性能的学科。
    几乎所有的经验都以数据的形式出现，因而机器学习的任务也就变成了基于已知数据构造概率模型，反过来再运用概率模型对未知数据进行预测与分析。

3.频率学派与贝叶斯学派对概率的不同观点
答案：1.在频率学派眼中，当重复试验的次数趋近于无穷大时，事件发生的频率会收敛到真实的概率之上。这种观点背后暗含了一个前提，那就是概率是一个确定的值，
        并不会受单次观察结果的影响。
       频率统计理论的核心在于认定待估计的参数是固定不变的常量，讨论参数的概率分布是没有意义的；而用来估计参数的数据是随机的变量，
        每个数据都是参数支配下一次独立重复试验的结果。由于参数本身是确定的，那频率的波动就并非来源于参数本身的不确定性，而是由有限次观察造成的干扰而导致。
        关键：采样分布、最大似然估计、置信区间、损失函数、经验风险
     2.P(H∣D)=P(D∣H)⋅P(H)/P(D)

2.频率视角下的机器学习
答案：


3.什么时候使用机器学习
答案：1.问题不能是完全随机的，需要具备一定的模式；2.问题本身不能通过纯计算的方法解决；3.有大量的数据可供使用
    机器学习的任务，就是使用数据计算出与目标函数最接近的假设，或者说拟合出最精确的模型。

4.机器学习分类
答案：1.按输入空间分：具体特征（concrete feature）、原始特征（raw feature）和抽象特征（abstract feature）
    2.按输出空间分：分类算法（classification）、回归算法（regression）和标注算法（tagging）
    3.按数据标签分：监督学习、无监督学习
    4.按学习策略分：批量学习（batch learning）集中处理所有的数据、在线学习（online learning）根据数据的不断馈入而动态地更新


5.计算学习理论
答案：对于机器学习来说，如果不能通过算法获得存在于训练集之外的信息，学习任务在这样的问题上就是不可行的
     PAC 学习理论的核心在于学习出来的模型会以较大概率接近于最优模型；
    1.概率近似正确”（Probably Approximately Correct, PAC）学习理论。机器学习利用训练集来选择出的模型很可能（对应名称中的“概率”）具有较低的泛化误差（对应名称中的“近似正确”）


6.模型的评估指标
答案：ROC 曲线

7.实验设计
答案：主要是建模，通过观察一个或多个因子对实验结果的影响，因子包括算法类型、超参数、数据集等。
    观察方法有控制变量法，完全交叉设计(不同因子之间系统化的变化对学习效果的影响。粗调 + 微调，先通过给每个因子较少可能取值，确定对学习结果影响较大的活跃因子，不活跃的抛弃。
        接下来聚焦活跃因子，增加取值数目进行微调观察因子之间的关系，可以使用响应面方法来降低计算开销。)

8.为什么要特征预处理
答案：佩德罗·多明戈斯教授的观点是：数据量(经过特征工程处理之后的精修数据)比算法更重要，特征工程是机器学习的关键

9.特征工程之前，数据的特征需要经过哪些必要的预处理
答案：特征缩放：保证所有的特征数值具有相同的数量级。使用方法有标准化、归一化
     数据偏度：描述概率分布非对称性的一个指标，如果概率不是正态分布可能存在异常点，检查是否有异常点存在
     异常点分析：异常点和正常点是否来源于不同的生成机制，从而具有不同的概率分布。如果对异常点所在的分布的采样数据较少，就不足以体现出分布的特性，导致单个数据点看起来显得突兀。
     异常点对算法是否敏感：如决策树，最直接的处理办法就是将异常点移除，但当数据集容量较小时，这种一刀切的方式会进一步减少可用的数据，造成信息的丢失，这时就需要采用名为“空间标识”（spatial sign）的数值处理方法。

     不存在异常点，有偏分布的一个明显特点是最大值和最小值之间相差较大。对它进行修正，也就是对数据进行去偏度处理的常用方法就是取对数变换。
        除了对数之外，求平方根和求倒数也是移除偏度的常见处理方式。

     缺失值处理：某些特征会压根儿没有取值，而是一片空白。
        1.将不完整的数据全部删除
        2.缺失值进行人为的赋值(K近邻算法、线性回归)
     比较特征之间的相关性：相关性较强，或者说具有共线性（collinearity），这时就可以删除掉其中的一个，这正是主成分分析的作用。
        移除一些特征有助于增强模型的可解释性，也可以降低计算中的开销。
     判别特征不具备区分度：某个特征在绝大多数数据中的取值都是相同的，那这个特征就没有存在的意义，因为它体现不出对于不同分类结果的区分度。
        1.特征取值的总数与样本数目的比例在 10% 以下
        2.出现频率最高的特征取值的出现频率应该在出现频率第二高的特征取值频率的 20 倍以上

======= 机器学习 ============

10.统计机器学习的原型
答案：线性回归，优点便于解释
    统计模型。其基本形式是 y=β0+β1x1+β2x2+⋯+βnxn+ϵ=， x是输入特征，β是模型参数


11.线性回归的正则化
答案：解决过拟合，通过动态调整估计参数的取值来降低模型的复杂度，以偏差的增加为代价来换取方差的下降。
    因为当一些参数足够小时，它们对应的属性对输出结果的贡献就会微乎其微，这在实质上去除了非相关属性的影响。

    解决方式：系数收缩方法，在损失函数（loss function）中添加正则化项
                LASSO回归（引入a||W||1）、岭回归
            边际化，计算出最优参数

12.LASSO回归
答案：1.初始猜测β1=0 和 β2=0开始，并迭代更新这些系数。r=y−Xβ
    β1(new)=β1(old)+ 1/n∑n(yi−(1+β1(old)X1i+β2(old)X2i))Xni
    当β1和β2的变化小于某个阈值时，我们认为模型已经收敛。



