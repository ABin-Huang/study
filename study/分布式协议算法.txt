算法基石：拜占庭容错算法，解决共识问题，不能最优选择策略以及性能问题

acid理论
 是传统数据库常用的设计理念，追求强一致性模型，通过原子性、隔离性、持久性实现数据一致性

BASE理论
 是对 CAP 中一致性和可用性权衡的结果，源于大规模互联网分布式系统实战的总结，基于CAP定理逐步演化而来
 核心思想是不推荐实现事务或强一致性，鼓励可用性和性能优先。根据业务场景特点，使用各种方案实现弹性的基本可用，
 以及数据的最终一致性。在NoSQL中广泛应用，是其系统设计的理论支撑。

1.有几种分布式事务
 柔性事务，基于base理论；刚性事务，基于cap，acid理论

2.如何实现最终一致性
 1)读时修复: 在读取数据时，检测数据的不一致，进行修复
 2)写时修复: 在写入数据，检测数据的不一致时(写操作的失败错误，发现不一致，然后通过重传修复数据的不一致)，进行修复，推荐同时实现自定义写一致性级别比如 All、Quorum、One、Any
 3)异步修复: 这个是最常用的方式，通过定时对账检测副本数据的一致性，并修复
 对比上述方案，推荐写时修复，不需要做数据一致性对比，性能消耗比较低

3.分布式共识Paxos
 当前最常用的一批共识算法都是基于它改进的，Fast Paxos 算法、Cheap Paxos 算法、Raft 算法等
 Paxos算法包含2个部分
  1)Basic Paxos 算法，描述的是多节点之间如何就某个值（提案 Value）达成共识；
  2)Multi-Paxos 思想，描述的是执行多个 Basic Paxos 实例，就一系列值达成共识。

4.Basic Paxos 的原理
 Basic Paxos 只能就单个值（Value）达成共识，一旦遇到为一系列的值实现共识的时候，它就不管用了

 提议者（Proposer）: 提议一个值，用于投票表决
 接受者（Acceptor）: 对每个提议的值进行投票，并存储接受的值
 学习者（Learner）: 被告知投票的结果，接受达成共识的值，存储保存，不参与投票的过程。一般来说，学习者是数据备份节点

 二阶段提交，进行共识协商
 1）准备阶段：提议者只将提案编号发送给接收者，接收者只能接受不小于该提案编号的信息，接收到大多数准备响应的提议者，才能发起接受请求进入第二阶段
 2）接受阶段：接受者最终接受最大提案编号携带的值
 3）当接受者通过了一个提案时，就通知给所有的学习者。当学习者发现大多数的接受者都通过了某个提案，那么它也通过该提案，接受该提案的值

 提案编号的大小代表着优先级，根据提案编号的大小，接受者保证三个承诺
 1）准备请求的提案编号，小于等于接受者已经响应的准备请求的提案编号，那么接受者将承诺不响应这个准备请求
 2）接受请求中的提案的提案编号，小于接受者已经响应的准备请求的提案编号，那么接受者将承诺不通过这个提案
 3）接受者之前有通过提案，那么接受者将承诺，会在准备请求的响应中，包含已经通过的最大编号的提案信息。通过告知提议者之前已经通过的最大编号的提案，
    提议者可以知道是否有冲突的提案需要处理。

5.chubby的Multi-Paxos算法
 解决一系列的值实现共识问题，每接收到一个值时，就执行一次 Basic Paxos 算法
 执行多次执行 Basic Paxos会存在几个问题
  1)如果多个提议者同时提交提案，可能出现因为提案编号冲突，在准备阶段没有提议者接收到大多数准备响应，协商失败，需要重新协商。你想象一下，
   一个 5 节点的集群，如果 3 个节点作为提议者同时提案，就可能发生因为没有提议者接收大多数响应（比如 1 个提议者接收到 1 个准备响应，
   另外 2 个提议者分别接收到 2 个准备响应）而准备失败，需要重新协商。
  2)2 轮 RPC 通讯（准备阶段和接受阶段）往返消息多、耗性能、延迟大。你要知道，分布式系统的运行是建立在 RPC 通讯的基础之上的，因此，
   延迟一直是分布式系统的痛点，是需要我们在开发分布式系统时认真考虑和优化的。

 如何解决上述问题，引入领导者和优化 Basic Paxos 执行
  1）通过引入主节点，实现领导者节点作为唯一提议者，这样就不存在多个提议者同时提交提案的情况，也就不存在提案冲突的情况，通过执行 Basic Paxos 算法，进行投票选举产生的
  2）由于领导者处于稳定状态，可以省掉准备阶段，直接进入接受阶段。因为不再需要通过准备请求来发现之前被大多数节点通过的提案，领导者可以独立指定提案中的值
  3）主节点会通过不断续租的方式来延长租期（Lease）。如果主节点故障了，那么其他的节点又会投票选举出新的主节点，也就是说主节点是一直存在的，而且是唯一的。

 缺点：为了实现了强一致性，读操作也只能在主节点上执行，所有的读请求和写请求都由主节点来处理

6.Raft算法
 Raft 算法属于 Multi-Paxos 算法，是现在分布式系统开发首选的共识算法。从本质上说，Raft 算法是通过一切以领导者为准的方式，实现一系列值的共识和各节点日志的一致
 三个重要步骤：
 领导者选举、
 日志复制：处理写请求的过程就是一个复制和应用（Apply）日志项到状态机的过程
  日志项格式包含用户数据（Command）、索引值（Log index）、任期编号（Term）
  日志复制消息格式包含当前要复制的日志项PrevLogEntry、当前要复制的日志项PrevLogTerm
  1.领导者进入第一阶段，通过日志复制（AppendEntries）RPC 消息，将日志项复制到集群其他节点上
  2.领导者接收到大多数的“复制成功”响应后，它将日志项应用到它的状态机，并返回成功给客户端; 否则返回错误给客户端
  3.follow节点接受日志后并不会马上将日志项应用到状态机，而是通过日志复制 RPC 消息或心跳消息实现，消息中包含了当前最大的，将会被提交（Commit）的日志项索引值
   这个优化，降低了处理客户端请求的延迟，将二阶段提交优化为了一段提交，降低了一半的消息延迟

  当follow节点落后日志复制时
   1.领导者通过日志复制 RPC 消息，发送当前最新日志项到跟随者
   2.跟随者在它的日志中，找不到匹配的PrevLogEntry和PrevLogTerm，发现日志同步不一致，拒绝接收新的日志项，并返回失败信息给领导者
   3.领导者会递减要复制的日志项的索引值，并发送新的日志项到该跟随者，重复2，3步骤知道匹配为止
   4.随者在它的日志中能匹配上，将日志应用到状态机
 成员变更(Raft 算法中唯一被优化和改进的部分)：领导者选举，建立在“大多数”的基础之上，集群成员发生了变化，可能同时存在新旧配置的 2 个“大多数”，出现 2 个领导者。
  两种方式：联合共识（Joint Consensus）、单节点变更（single-server changes）
  核心概念：配置，表示集群是哪些节点组成的，是集群各节点地址信息的集合
  单节点变更方式
   1.领导者（节点 A）向新节点（节点 D）同步数据；
   2.领导者（节点 A）将新配置[A, B, C, D]作为一个日志项，复制到新配置中所有节点（节点 A、B、C、D）上，然后将新配置的日志项应用（Apply）到本地状态机，完成单节点变更。
   缺点：在分区错误、节点故障等情况下，如果我们并发执行单节点变更，那么就可能出现一次单节点变更尚未完成，新的单节点变更又在执行，导致集群出现 2 个领导者的情况
   解决方案：开启NO_OP日志项，作用是作为一个“同步信号”，确保领导者和追随者在执行关键操作之前处于一致的状态。只有在NO_OP日志项应用后，才执行成员变更请求，从而保证集群的稳定性和一致性

7.哈希算法，提升读写性能，分散读写压力
 本质上是一种路由寻址算法
 哈希算法：对节点的数量进行取模运算，实现简单，容易理解，但是当需要变更集群数时，大部分的数据都需要迁移，重新映射，数据的迁移成本是非常高
 一致哈希(解决扩缩容)：也是取模运算，对 2^32 进行取模运算，空间组织成一个虚拟的圆环，通过主机名作为参数进行hash，确定其在哈希环上的位置，沿着哈希环顺时针“行走”，遇到的第一节点就是 key 对应的节点
    缺点：节点太少，容易因为节点分布不均匀造成数据访问的冷热不均
 虚拟节点哈希(解决冷热不均): 将哈希环上的位置映射到虚拟节点上












