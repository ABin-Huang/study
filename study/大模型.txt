1.什么是AIRC 系统
答案：策略建模、数据工程和模型工程

2.特征工程
回答：给数据集建模：通过将数据特征投影到高维空间，用非线性处理、特征组合处理和归一化处理等特征处理方法，让你的特征更好地服务于模型。
比如：人的兴趣，插花、篮球、绘画之间的关系

3.人工智能在学术上的三大学派
答案：符号主义学派、连接主义学派和行为主义学派。其中的代表分别是知识图谱、深度学习和强化学习。

对比学习（只需要得到样本之间的“相似度”就能完成训练，连接主义），监督学习（给参考答案，连接主义）
强化学习的核心思想，是利用感知和行动的闭环进行学习。行为主义

建知识图谱的三个主要步骤：知识抽取、知识融合和知识加工

4.构建大语言模型系统
答案：
1.利用外部记忆
2.高效存储和检索外部记忆 （知识表征技术-倒排索引,没有语义信息，受多义词干扰、嵌入表征，语义信息，无法表示段落和上下文内容、知识图谱，结构化学习和语义关联，难形成全文语义）


5.如何获取大模型训练数据
答案： Self-Instruct 方法
  1.使用一组人工编写的指令（本例中为 175 条）来初始化任务池，并随机选择一些指令。
  2.利用预训练的大型语言模型（如 GPT-3）来确定任务的类别。
  3.给定新的指令，让预训练的语言模型生成回应。
  4.在将回应添加到任务池之前，进行回应的收集、修剪和过滤。

6.GPT如何预训练模型
答案：基于Decoder Only 的 Transformer 架构，使用Word2Vec 词向量技术，将文本转换为向量，然后输入到模型中进行训练。结合ELMO 和GPT 的优点，使用双向语言模型，在训练中引入上下文信息。
对于完形填空等问题上显然存在一定的天然劣势。

7.BERT预训练模型
答案：基于Encoder Only 的 Transformer 架构，采用了类似 ELMo 的双向语言模型，同时利用上文和下文信息进行建模预测。

8.注意力机制和自注意力机制的区别是什么
答案：注意力机制是让模型在处理序列数据时，能够关注到不同位置的信息（输入与输出之间的关联）。
  自注意力机制是注意力机制的一种，它可以让模型在处理序列数据时，能够关注到不同位置的信息。（词与词之间的内部关联性），对翻译文本、语音识别、文本摘要、图像描述等任务效果显著。

9.大模型为什么会产生智能
答案：涌现 1.参数规模的影响，不同类型的任务在参数增加时，表现出了三种不同的特征，分别是伸缩、涌现和 U 形曲线效应。

两个最典型的涌现能力表现——ICL 和 CoT。
In Context Learning (ICL)：大语言模型从少量的示例中学习，而无需微调参数的能力。
思维链 (CoT)：大语言模型能够理解和执行复杂的推理过程。