共识算法:Paxos、ZAB、Raft(Leader 选举、日志同步、安全性) 等
 1.Leader选举，Leader故障后集群能快速选出新Leader；
 2.日志复制， 集群只有 Leader 能写入日志， Leader负责复制日志到Follower节点，并强制Follower节点与自己保持相同；
 3.安全性，一个任期内集群只能产生一个 Leader、已提交的日志条目在发生Leader选举时，一定会存在更高任期的新 Leader 日志中、各个节点的状态机应用的任意位置的日志条目内容应一样等。
 建分布式系统的核心要素角度:数据模型、复制、共识算法、API、事务、一致性、成员故障检测等方面

 角色，角色的作用
 Follower，跟随者， 同步从 Leader 收到的日志，etcd 启动的时候默认为此状态；
 Candidate，竞选者，可以发起 Leader 选举；
 Leader，集群领导者， 唯一性，拥有同步日志的特权，需定时广播心跳给 Follower 节点，以维持领导者身份。

 多副本复制机制：主从复制(全同步复制、异步复制、半同步复制)复制算法、去中心化复制(共识算法)

 2.leader选举：当节点认为集群中没有leader节点时会发起选举，Follower先发起PreVote(默认关闭,避免节点长时间无法连通网络问题引起的leader选举)，进入 PreCandidate 状态，不自增任期号，若获得集群多数节点认可进入 Candidate 状态，并发起竞选Leader投票，若获得集群多数节点的支持后，就可转变成Leader节点
  
  2.1 初始化状态
   所有节点初始状态为 Follower，每个节点维护以下状态：
   currentTerm：当前任期编号，初始为 0，单调递增。
   votedFor：当前任期内投票给的节点 ID，初始为 null。
   log[]：日志条目，每个条目包含命令和创建该条目的 Leader 的任期编号。
  2.2 触发选举
   Leader 定期向 Follower 发送心跳（AppendEntries RPC），以维持其领导地位
   如果 Follower 在预定的超时时间（election timeout，随机值，通常在 150-300ms 之间）内未收到 Leader的心跳，它会转变为Candidate并触发选举。
  2.3 选举过程
   增加任期编号：Candidate 将自己的 currentTerm 加 1。
   投票给自己：Candidate 投票给自己，并将 votedFor 设置为自己的 ID。
   请求投票：Candidate 向其他节点发送 RequestVote RPC，请求投票。
   RPC 包含 term、candidateId、lastLogIndex 和 lastLogTerm，用于比较日志的一致性。
   投票逻辑：
     如果接收方的 currentTerm 大于请求方的 term，则拒绝投票。
     如果接收方已经为当前任期投票给其他节点，则拒绝投票。
     如果接收方的日志比请求方更完整，则拒绝投票。
     投票结果：
     如果 Candidate 收到超过半数节点的投票，则成为新的 Leader。
     如果没有获得多数票，Candidate 保持 Candidate 状态，等待随机超时后重新发起选举。
     如果在选举过程中收到其他节点的 AppendEntries RPC，且其 term 不小于当前节点的 term，则 Candidate 重新变为 Follower。
  2.4 避免投票分裂
   为了避免多个 Follower 同时超时并发起选举，Raft 使用随机超时时间（150-300ms）来减少同时触发选举的概率。
   如果出现投票分裂（即没有节点获得多数票），Candidate 会增加任期编号并重新发起选举。
  3. 日志一致性检查
   在投票过程中，Raft 会比较日志的一致性：
   如果 Candidate 的日志比投票节点的日志更完整（lastLogIndex 和 lastLogTerm 更大），则投票节点会投票给 Candidate。
   这种机制确保只有日志最新的节点能够成为 Leader，从而保证了日志的一致性。
  4. 选举完成后的行为
   当一个节点成为 Leader 后，它会立即向所有 Follower 发送心跳（AppendEntries RPC），以维持其领导地位。
   如果其他节点在选举过程中成为 Leader，Candidate 会重新变为 Follower。
  5. 选举超时的权衡
   较短的选举超时时间可以更快检测到 Leader 的失败，但可能导致更多的无效选举。
   较长的选举超时时间可以减少无效选举，但会增加 Leader 失败后的不可用时间。

 3.日志复制--确保集群中所有节点日志一致性的关键机制
  1. 客户端请求提交到 Leader
    当客户端向 Leader 发起请求（如写入操作）时，Leader会将请求转换为一条日志条目（Entry），并将其追加到本地日志中。此时，该日志条目处于未提交状态（uncommitted），Leader不会立即更新本地数据。
  2. Leader 将日志条目复制到 Follower
    Leader 会通过 AppendEntries RPC 将新的日志条目并行地发送给所有 Follower 节点。AppendEntries RPC 包含以下内容：
     当前 Leader 的任期号（Term）。
     日志条目（Entries）。
     前一个日志条目的索引值（PrevLogIndex）和任期号（PrevLogTerm），用于日志一致性检查。
  3. Follower 处理日志条目
    Follower 收到 AppendEntries RPC 后，会执行以下操作：
     日志一致性检查：
        如果 Follower 的日志与 Leader 不一致（如 PrevLogIndex 或 PrevLogTerm 不匹配），Follower 会拒绝该日志条目，并返回拒绝响应。
     追加日志条目：
        如果一致性检查通过，Follower 会将日志条目追加到本地日志中，并返回成功响应。
  4. Leader 确认日志提交
    Leader收到多数Follower的成功响应后，会将该日志条目标记为已提交（committed），并将其应用到本地状态机中。随后，Leader     会通过心跳消息或后续的 AppendEntries RPC 将最新的提交索引（Commit Index）同步给所有 Follower。
  5. Follower 应用日志
    Follower 在收到心跳消息或新的 AppendEntries RPC 后，会检查 Leader 的提交索引。如果 Follower    发现本地日志中存在未应用的已提交日志条目，它会将这些条目应用到本地状态机中。
  6. 处理日志不一致
    如果 Follower 拒绝了 Leader 发送的日志条目，Leader 会尝试以下操作：
     回退日志索引：
        Leader 会将 Follower 的 NextIndex 回退到 Follower 返回的拒绝提示（RejectHint）位置。
     重新发送日志：
        Leader 会重新发送从 NextIndex 开始的日志条目，直到 Follower 接受为止。
  7. 优化：心跳消息中的提交索引
    Raft 协议通过优化减少了消息延迟。Leader 的心跳消息（MsgHeartbeat）中会包含最新的提交索引，Follower   可以通过心跳消息直接了解需要应用的日志条目，而无需额外的提交通知。

    NextIndex和MatchIndex是Leader维护的数组
    NextIndex：Leader下一次将要发送给该 Follower 的日志条目的索引，当Follower成功追加日志后，Leader会将NextIndex增加1
    MatchIndex：Follower已成功复制的最大日志条目索引，Leader 确认多数Follower的MatchIndex，该日志条目才会被标记为已提交

 4.安全性-- Leader 完全特性和只附加原则、日志匹配等安全机制
    Leader 完全特性：某个日志条目在某个任期号中已经被提交，那么这个条目必然出现在更大任期号的所有Leader中。
    只附加原则：Leader只能追加日志条目，不能删除已持久化的日志条目
    日志匹配：Follower节点会检查相同索引位置的任期号是否与Leader一致，一致才能追加

 冲突解决机制：
  1.找到一致的日志位置，Leader获取nextIndex，想follower发送下一条需要发送的日志条目索引，如果Follower拒绝（例如，PrevLogIn  或 PrevLogTerm 不匹配），
    Leader 会递减 nextIndex，并重新发送。直到找到 Leader 和 Follower 都认可的日志位置
  2.删除冲突日志，Follower会删除该位置之后的所有冲突日志条目，Leader然后将自己从该位置之后的所有日志条目发送给Follower，确保  Follower的日志与Leader一致


 5.鉴权机制
  密码认证(内外环境)：blowfish 算法，基于明文密码、随机分配的salt、自定义的 cost、迭代多次计算得到一个 hash 值，并将加密算法版本、salt 值、cost、hash 值组成一个字符串，
        作为加密后的密码。认证时，根据用户名获取密码信息，根据请求的明文密码，计算出最终的hash值，判断计算结果与存储一致。
    优化：减少加密次数。使用Simple Token 和 JWT Token
    Simple Token：当一个用户身份验证通过后，生成一个随机的字符串值，使用map存储用户和Token映射关系，token默认有效期5分钟，token不存储任何信息，
        导致客户端无法提前去规避因Token失效导致的请求报错。

  证书认证

  鉴权配置注意事项：
   1.使用最小权限原则
   2.合理设计权限模型，尽量通过角色继承来简化权限管理
   3.鉴权日志不清晰，通过设置日志级别为debug
   4.在版本升级前，备份当前的用户和角色配置。

功能设计：
 1.高可用，支持多节点部署，读取和写入
 2.数据一致性(共识算法)，避免读取数据过期数据
 3.占用空间少，只存储关键元数据
 4.提供增删查改功能，监听数据变化机制，及时推送最新变更数据，提升服务可用性
 5.可维护性，遇到bug或人为操作错误导致节点宕机，或者及新增、替换节点，可通过API实现平滑地变更成员节点信息

 zookeeper缺点
  1.不支持通过 API 安全地变更成员，需要人工修改一个个节点的配置，并重启进程。变更姿势不正确，则有可能出现脑裂等严重故障
  2.Java编写的，部署较繁琐，占用较多的内存资源。
  3.RPC的序列化机制用的是 Jute，无法使用 curl 之类的常用工具与之互动



串行读：直接读取状态机
线性读(3.1)：需要经过 Raft 协议模块，反应的是集群共识
  1.从leader获取最新已提交日志索引(committed index)
  2.为防止脑裂等异常，leader向follower发送心跳确认，半数以上确认才能将已提交索引返回节点
  3.从节点等待，直到状态机已提交索引大于等于leader索引，返回结果告诉从节点是否可以去状态机访问数据

 3.2：读请求通过走一遍 Raft 协议保证一致性， 这种 Raft log read 机制依赖磁盘 IO， 性能相比 ReadIndex 较差。

 mvvc由treeIndex和boltdb组成
 从 treeIndex 中获取 key 的keyIndex，包含所有版本号，再以key:version作为 boltdb 的 key，从 boltdb 中获取其 value 信息。


type keyIndex struct {
    key         []byte // key 的值
    modified    Revision // 最后一次修改的 main revision
    generations []generation // 每个generation保存创建到删除的版本，删除操作后新增一个空的generation
}
 
type Revision struct {
    // 就是 revision 的值，同个事务相同，不同事务不同
    Main int64
    // 同一个事务中递增
    Sub int64
}
 
 
// generation 保存了 key 的历史版本
type generation struct {
    ver     int64 // 版本号
    created Revision // generation结构创建时的版本号
    revs    []Revision // 保存了 key 的历史 revision
}

如何实现MVCC多版本控制
 1.由B-tree实现的tree Index,节点keyIndex由key、modified revision、generation组成
 2.boltdb基于B+ tree实现，key为revision{2,0}，value由key、value、create_revision、mod_revision、version、lease 组成

 新增流程
   1.根据key查询tree Index获取KeyIndex，如果查不到构建boltdb的key，value写入boltdb和buffer，
     再创建KeyIndex，更新写入到treeIndex，backend异步事务提交，默认1万事务
 更新流程
   1.根据key查询tree Index获取KeyIndex，设置版本号，修改次数等信息后构建boltdb的key，value写入boltdb和buffer，
     再更新KeyIndex，更新写入到treeIndex，backend异步事务提交，默认1万事务
 删除流程
   1.根据key查询tree Index获取KeyIndex，生成的 boltdb key 版本号{4,0,t}追加了删除标识，
     boltdb value变成只含用户key的KeyValue结构体。treeIndex中的keyIndex对象追加一个空的generation对象，表示此索引对应的key被删除了
   2.再次查询时treeIndex 模块根据 key hello 查找到 keyindex 对象后，若发现其存在空的 generation 对象，
     并且查询的版本号大于等于被删除时的版本号，则会返回空。
   3.key打上删除标记t后有哪些用途呢？什么时候会真正删除它呢
      删除 key 时会生成 events，Watch 模块根据 key 的删除标识，会生成对应的Delete 事件。
      当重启etcd，遍历 boltdb 中的 key 构建 treeIndex 内存树时，需要知道哪些 key 是已经被删除的
      真正删除treeIndex 中的索引对象、boltdb 中的 key 是通过压缩 (compactor) 组件异步完成。

 并发读：创建读事务对象时，它会全量拷贝当前写事务未提交的 buffer 数据，并发的读写事务不再阻塞在一个 buffer 资源锁上
  treeIndex的ReadTxn，Backend/boltdb的ReadTx

写请求
1.客户端使用gRPC API发起写请求
2.Quota--检查当前 etcd db 大小加上key-value 大小之和是否超过了配额,Preflight Check为了保证集群稳定性，避免雪崩,会做一些简单的判
断,如已提交的日志索引超过5000、token无效、包大于1.5M。通过检查后生产唯一ID，向Raft发起Proposal，默认等待7秒(5 秒磁盘 IO 延时 +2*1 秒竞选超时时间),收到结果后将日志条目添加到先进先出（FIFO）调度队列。
3.raft--如果是Follower节点，转发给leader处理写请求。leader会将put提案消息广播给集群各个节点，同时集群 Leader 任期号、投票信息、已提交索引、提案内容持久化到WAL。当一半以上节点持久化此日志条目后，通知原来节点数据已提交。
4.Apply--按入队顺序，异步、依次执行提案内容。
  刚开始执行就宕机，重启时如何恢复日志。从WAL解析Raft日志条目，追加到Raft日志中，并重放日志提案给Apply执行，在执行提案内容前先查询consistent index是否存在，不存在才新增同时无 db 配额满告警,进入到 MVCC 模块。
5.MVCC--内存索引模块 treeIndex,持久化存储boltdb 模块
  5.1 从boltdb获取key的最大版本号 currentRevision,然后从 treeIndex 模块中查询 key 的创建版本号、修改次数信息, 更新treeIndex信息，这些信息将填充到 boltdb 的 value 中，同时将用户的 hello key 和 revision 等信息存储到B-tree       
  5.2 更新内存数据后，会先将数据写入到 bucket buffer缓存，异步机制定时（默认每隔 100ms）将批量事务一次性提交（pending 事务过多才会触发同步提交）               


WAL日志结构: 
    Len Field、
    Type(文件元数据记录、日志条目记录、状态信息记录、CRC 记录、快照记录)
      文件元数据记录包含节点 ID、集群 ID 信息，它在 WAL 文件创建的时候写入；
      日志条目记录包含 Raft 日志信息，如 put 提案内容；
      状态信息记录，包含集群的任期号、节点投票信息等，一个日志文件中会有多条，以最后的记录为准；
      CRC 记录包含上一个 WAL 文件的最后的 CRC（循环冗余校验码）信息， 在创建、切割 WAL 文件时，作为第一条记录写入到新的 WAL 文件， 用于校验数据文件的完整性、准确性等；
      快照记录包含快照的任期号、日志索引信息，用于检查快照文件的准确性。
    CRC、
    Data

怎么避免数据丢失crash-safe
WAL日志

幂等性
当更新数据和更新 consistent index到boltdb操作都成功时，使用consistent index字段来存储系统当前已经执行过的日志条目索引。

Apply 模块在执行提案内容前，首先会判断当前提案是否已经执行过了，如果执行了则直接返回，若未执行同时无 db 配额满告警，则进入到 MVCC 模块，开始与持久化存储模块交互

事务提交包含 B+tree 的平衡、分裂，将 boltdb 的脏数据（dirty page）、元数据信息刷新到磁盘
优化：合并再合并
  1.调整 boltdb 的 bucket.FillPercent 参数，使每个 page 填充更多数据，减少 page 的分裂次数并降低 db 空间。
  2.合并多个写事务请求,是异步机制定时（默认每隔 100ms）将批量事务一次性提交（pending 事务过多才会触发同步提交）
  3.第二点会影响数据实时性，通过引入bucket buffer缓存来保存暂未提交的事务数据,定期将bucket buffer同步到 BoltDB

lease
 使用场景
  1.服务发现与保活：动态管理服务实例的生命周期
  2.分布式锁：协调多个实例对共享资源的访问
  3.缓存管理：自动清理过期的缓存数据
  4.配置管理：动态更新和清理配置信息
 lessor
   管理lease的生命周期：
    创建(持久化到BoltDB,并存储在leaseMap，Leader节点会被加到最小堆)
    续租(Leader节点接收续租请求后，会更新 Lease 的过期时间，并通过Raft协议将更新同步到其他节点)
    撤销(定期检查,获取最小堆堆顶检查是否过期,Lease 过期触发撤销操作，删除绑定的键值对,通过 Raft 日志同步到其他节点)
       Checkpoint 时间间隔为 5 分钟，解决 Leader 切换时的 Lease 状态一致性问题，新 Leader 可能无法正确重建 Lease 的剩余 TTL，导致 Lease 永远无法删除，进而引发资源泄漏
       查找需要同步的lease，判断下一次checkpoint时间(通常是TTL的1/3, 1/2)小于当前时间，
   RevokeExpireLease定时任务协程:维护leaseExpireNotifierHeap小顶堆，根据key的TTL排序
   CheckpointScheduledLeases定时任务协程：维护CheckPointHeap小顶堆，将Lease的剩余TTL同步到其他节点
 Lease：包含LeaseID、TTL（存活时间）、过期时间等信息

Watch：如何高效获取数据变化通知
 1.client 获取事件的机制
   轮询：v2版本中使用HTTP/1.x,每个watcher对应一个TCP，使用长连接定时轮询server，watcher太多server的QPS高，socket，内存占用资源多
        v3版本中HTTP/2的gRPC，实现了连接多路复用，一个 client/TCP 连接支持多 gRPC Stream， 一个 gRPC Stream 又支持多个 watcher，
          当 watch 连接的节点故障，clientv3 库支持自动重连到健康节点，并使用之前已接收的最大版本号创建新的 watcher，避免旧事件回放
   流式推送：v3
 2.事件是如何存储的，会保留多久，watch 命令中的版本号具有什么作用
    MVCC 机制则将历史版本保存在磁盘中，避免了历史版本的丢失
    版本号是 etcd 逻辑时钟，当 client 因网络等异常出现连接闪断后，通过版本号，它就可从 server 端的 boltdb 中获取错过的历史事件，而无需全量同步，它是 etcd Watch 机制数据增量同步的核心。
 3.出现网络异常后，事件堆积，server如何处理事件，监听不存在的版本号，会发生什么  本质：可靠的事件推送机制
   需要理解watch的工作流程，客户端和服务端是怎么协作完成
   serverWatchStream：recvLoop、sendLoop
   WatchStream
   WatchableKV(watchableStore)
   syncWatchersLoop
   syncVictimsLoop
   unsynced watcher
   victim watcher
   synced watcher
 4.海量监听key变化，如何根据变化的key快速找到对应的watch
   使用map记录了监听单个key的watcher，监听多个key使用区间树
   etcd首先需要从map查找是否有watcher监听了单key，再从区间树找出与此key相交的所有区间，然后从区间的值获取监听的watcher集合

5.etcd的3种事务机制
 1)读提交：只有数据回写到buffer才能被读取，是当前读
 2)可重复读：创建读缓存，将数据写入读缓存，后续查询都先查找读缓存
 3)串行化快照：在事务刚开始时，首先获取 etcd 当前的版本号 rev，事务中后续发出的读请求都带上这个版本号 rev，
    确保事务提交时，你读写的数据都是最新的，未被其他人修改，也就是要增加冲突检测机制。当事务提交出现冲突的时候依赖 client 重试解决，安全地实现多 key 原子更新。

6.boltdb的数据结构，在member/snap/db 的文件， page size 为 4KB。
 每个页面的结构由以下部分组成：
  页 ID（id）：标识页面的唯一 ID。
  页类型（flags）：标识页面的类型（如 Meta Page、Branch Page、Leaf Page 等）。
  数量（count）：页面中存储的元素数量（仅在 Branch 和 Leaf 页面中生效）。
  溢出页数量（overflow）：当页面数据存放不下时，需要申请的额外连续页面数量。
  页面数据起始位置（ptr）：指向页面的载体数据，如 Meta Page、Branch/Leaf 等页面的内容。

 2个meta page：元数据页
   Magic Number（4 字节）：用于识别文件是否为 BoltDB 文件，其值为 0xED0CDAED。
   Version（2 字节）：表示 BoltDB 的版本号。
   Page Size（4 字节）：指定每个页面的大小，通常为 4KB。
   Flags（4 字节）：用于存储一些标志位，通常为 0。
   Root Bucket（16 字节）：描述了 BoltDB 的根 bucket 信息，包括根 bucket 的位置和相关信息。
   Freelist Page ID（8 字节）：指向空闲页管理页的页面 ID，用于记录哪些页面是空闲的。
   Total Page ID（8 字节）：表示当前数据库文件中已分配的总页面数。
   Last Write Transaction ID（8 字节）：记录上一次写事务的 ID。
   Checksum（8 字节）：用于校验文件的完整性，检测文件是否损坏。
 freelist page: 空闲页管理页
 branch page: B+ tree 索引节点页
 leaf page: B+ tree 叶子节点页
 2个free page：空闲页

7. boltdb 是如何根据元数据页面信息快速找到你的bucket 和 key-value 数据呢？
 1)读取meta page,通过 txid 比较两页，选择 txid 更大且校验和有效的页,获取根 bucket 的根页面 ID
 2)根据根页面 ID，找到对应的 Leaf Page 或 Branch Page
    如果根页面是 Leaf Page，走第四步
    如果根页面是 Branch Page，则需要递归查找。走第三步
 3)在 Branch Page 中，通过二分查找匹配目标 key，获取其子节点的页面 ID，递归访问子节点页面，直到找到目标的 Leaf Page
 4)在 Leaf Page 中，通过二分查找匹配目标 key
     如果找到匹配的 key，返回对应的 value 数据。
     如果 key 不存在，返回未找到的提示。

8.boltdb Open API 的原理
 1)首先它会打开 db 文件并对其增加文件锁，目的是防止其他进程也以读写模式打开它后，操作 meta 和 free page，导致 db 文件损坏
 2) boltdb 通过 mmap 机制将 db 文件映射到内存中，并读取两个 meta page 到 db对象实例中，
 然后校验 meta page 的 magic、version、checksum 是否有效，若两个meta page 都无效，那么 db 文件就出现了严重损坏，导致异常退出

9.boltdb Put 原理
 1)获取meta page 中记录 root bucket的 root page，从 root page 递归搜索到对应的叶子节点page 面，返回 key 名称、leaf 类型
 2)如果 leaf 类型为 bucketLeafFlag，且 key 相等，说明已经创建过结束请求。否则往 B+ tree 中添加一个 flag 为 bucketLeafFlag 的 key，
   key称为 bucket name，value 为 bucket 的结构。
 3)创建完 bucket 后,以通过 bucket 的 Put API 发起一个 Put 请求更新数据,根据子 bucket 的 root page，从 root page 递归搜索此 key到 leaf page，
   如果没有找到，则在返回的位置处插入新 key 和 value。只是将值更新到 boltdb 的内存 node 数据结构里，并未持久化到磁盘中。
 4)事务提交(真正持久化数据到磁盘)，进行删除节点后重平衡操作、分裂操作、持久化freeList、持久化dirty page、持久化meta page
    注意：在 etcd v3.4.9 中，为了优化写性能等，freelist 持久化功能是关闭的。etcd 启动获取 boltdb db 对象的时候，boltdb 会遍历所有 page，构建空闲页列表

10.事务提交过程中若持久化 key-value 数据到磁盘成功了，此时突然掉电，元数据还未持久化到磁盘，会发生什么
 1)数据一致性问题: 元数据页（Meta Page）包含数据库的根 bucket 信息、事务 ID 等关键信息。如果元数据未持久化，数据库在恢复时可能无法正确识别哪些数据是有效的，哪些是未完成的事务
 2)数据丢失或损坏: 如果元数据页未持久化，数据库在恢复时可能无法正确解析 B+ 树的结构，导致部分数据丢失或损坏。
 数据恢复机制：BoltDB在启动时会检查两个元数据页的一致性。如果发现不一致，会尝试从另一个页恢复数据。
  检查元数据页：检查两个元数据页的一致性，选择一个有效的元数据页用于恢复。
  校验数据完整性：通过元数据页中的事务 ID 和校验码，校验数据的完整性。
  修复 B+ 树：根据元数据页中的信息，修复 B+ 树的结构，确保数据的一致性。

11.历史版本回收机制
 原理： 会更新当前 server 已压缩的版本号，并将耗时昂贵的压缩任务保存到FIFO 队列中异步执行。当任务执行时，压缩treeIndex模块中的keyIndex索引，
   其次会遍历 boltdb 中的 key，删除已废弃的 key。
 1)时间周期性压缩
   根据配置的模式，创建 periodic Compactor异步的获取、记录过去一段时间的版本号，将配置压缩间隔时间划分10份，
   每隔一份通过 etcd MVCC 模块的接口获取当前的 server 版本号，追加到 rev 数组中。
 2)版本号压缩，使用场景写请求比较多
   创建 revision Compactor。revision Compactor 会根据你设置的保留版本号数，每隔 5 分钟定时获取当前 server 的最大版本号，
   减去你想保留的历史版本数，通过 etcd server 的 Compact 接口发起如下的压缩
 3)基于 etcd 的 Compact API实现，在业务逻辑代码中、或定时任务中主动触发压缩操作。

 compact原理
  1)Compact 请求经过 Raft 日志同步给多数节点后，etcd 会从Raft 日志取出 Compact 请求，应用此请求到状态机执行
  2)会检查 Compact 请求的版本号rev 是否已被压缩过，若是则返回 ErrCompacted 错误给 client
   检查 rev 是否大于当前 etcd server 的最大版本号，若是则返回 ErrFutureRev 给 client
  3)检查通过后，通过 boltdb 的 API 在 meta bucket 中更新当前已调度的压缩版本号scheduledCompactedRev，将压缩任务追加到 FIFO Scheduled
  4) 异步压缩任务首先会克隆一个 B-tree,压缩 treeIndex 模块中的 keyIndex 索引，移除小于等于压缩版本号的历史版本号,
    通过一个 map 记录 treeIndex 中有效的版本号返回给 boltdb 模块使用
  5)scheduleCompaction任务,从 0 到 CompactedRev 遍历 boltdb 中的所有 key，通过 treeIndex 模块返回的有效索引信息，
    判断这个 key 是否有效，无效则调用 boltdb 的 delete 接口将 key-value 数据删除。更新当前 etcd 已经完成的压缩版本号(finishedCompactRev)，
    将其保存到 boltdb 的 meta bucket 中。(为了不影响正常读写请求,分批完成 boltdb key 的删除操作，控制每次遍历、删除的 key 数，默认为100，每批间隔 10ms)
 注意：Watch 特性中的历史版本数据同步，依赖于 MVCC 中是否还保存了相关数据，需要精细化的控制历史版本数












